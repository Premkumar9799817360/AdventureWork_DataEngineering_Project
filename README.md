# ğŸš€ Azure End-to-End Data Engineering Project  

### ğŸ“˜ Project Overview  
This is a complete **End-to-End Data Engineering project built on Microsoft Azure**. The project uses the Medallion Architecture (Bronze, Silver, Gold layers) to process and transform data efficiently.

ğŸ› ï¸ Technologies Used

- â˜ï¸ **Azure Data Lake Storage Gen2** - Data storage
- ğŸ§© **Azure Data Factory** - Data extraction and pipeline orchestration
- ğŸ”¥ **Azure Databricks** - Data transformation
- ğŸ“Š **Azure Synapse Analytics** - Data warehousing and analytics
- âš™ï¸ **Apache Spark & PySpark** - Big data processing
---

## ğŸ¯ Project Goal  
To build a **robust and scalable data pipeline** using Azure services and implement a **Medallion Architecture** (Bronze, Silver, and Gold layers) using the **AdventureWorks dataset**.


## ğŸ—‚ï¸ Dataset Used  
ğŸ“¦ **AdventureWorks Dataset**  

I used the **AdventureWorks** dataset because it contains multiple CSV files with diverse data such as:  
- ğŸ·ï¸ Product  
- ğŸ“ Product Categories  
- ğŸ’µ Sales  
- ğŸ™â€â™‚ï¸ Customers  
- ğŸ”„ Returns  



---
ğŸ—ï¸ **Architecture**

The project follows the Medallion Architecture with three layers:

- Bronze Layer ğŸ¥‰ - Raw data storage
- Silver Layer ğŸ¥ˆ - Cleaned and transformed data
- Gold Layer ğŸ¥‡ - Business-ready analytics data
  
![Project Architecture](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Project%20Architecture.png)
---

## ğŸ§© Architecture: Medallion Approach  

### ğŸ¥‰ Bronze Layer (Raw Data)
This project focuses on **data extraction** using a **REST API**.  
I uploaded all the datasets to my **GitHub repository** so that they can be accessed through **HTTPS links**.  
Using **Azure Data Factory (ADF)**, I built a **dynamic pipeline** that connects to GitHub and extracts data from all CSV files automatically.  
Each dataset serves a different purpose, and the pipeline is designed to handle multiple files efficiently.  

### ğŸ”§ Pipeline Details  
The pipeline in Azure Data Factory includes the following activities:  
- **Lookup Activity** â€“ reads the list of available files from GitHub  
- **Copy Data Activity** â€“ copies each dataset through HTTPS connection  
- **ForEach Activity** â€“ loops through all file names dynamically and extracts each dataset  

ğŸ“¸ *Azure Data Factory Pipeline:*  
![ADF Pipeline](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Pipeline_workflow.jpg)

ğŸ“¸ *Successful Pipeline Run:*  
![ADF Pipeline Success](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Pipeline_Successful_Run.jpg)

After the pipeline ran successfully, all extracted datasets were automatically saved in my Azure Data Lake Gen2 storage under the Bronze layer for further processing.

ğŸ“¸ *Bronze Layer Container:*
![Bronze Layer](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Bronze_Layer_container.jpg)

---

### ğŸ¥ˆ Silver Layer (Transformation)
After loading data into the Bronze Layer, I connected **Azure Databricks** to **ADLS Gen2** for transformations.

I created a **Silver Notebook** in Databricks to:  
- ğŸ§® Load raw data from the Bronze Layer  
- ğŸ§¹ Apply data cleaning and transformations  
- ğŸ’¾ Save transformed data back to **Silver Layer** in **Parquet format** (best for big data)

ğŸ“¸ *Silver Layer Data in ADLS Gen2:*  
![Silver Layer Storage](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Silver_layer_container.jpg)

ğŸ“¸ *Silver Layer Data*  
![Silver Layer Storage Output](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Silver_layer_container_file_output.jpg)

---

### ğŸ¥‡ Gold Layer (Analytics & Warehouse)
The final step is the **Gold Layer**, where I used **Azure Synapse Analytics** to query and analyze the transformed data.  

Steps:  
1. ğŸ§± Created a **Gold Schema** in Synapse  
2. ğŸ”— Used **OPENROWSET** and **BULK functions** to load Silver data into Synapse  
3. ğŸ“Š Created **External Tables** and **Views** using **CETAS (Create External Table As Select)**  

ğŸ“¸ *SQL Query using OPENROWSET & BULK:*  
![SQL Query in Synapse](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Sql_script.jpg)

ğŸ“¸ *Gold Layer Storage (ADLS Gen2):*  
![Gold Layer Storage](https://github.com/Premkumar9799817360/AdventureWork_DataEngineering_Project/blob/main/Project%20Image/Gold_layer_container_output.jpg)

---


## ğŸ§© Key Learnings  
ğŸ’¡ Learned how to connect and integrate multiple Azure services  
ğŸ’¡ Built a **Dynamic Pipeline** for multiple datasets  
ğŸ’¡ Understood **Medallion Architecture** deeply (Bronze, Silver, Gold)  
ğŸ’¡ Improved my knowledge of **ADF, ADLS, Databricks, and Synapse**  
ğŸ’¡ Gained hands-on experience in **data transformation and optimization**

---

## ğŸ Conclusion  
This project helped me gain strong practical knowledge of Microsoft Azure Data Engineering tools.  
I learned how to manage, extract, transform, and store large datasets using a real-world data architecture.

---

